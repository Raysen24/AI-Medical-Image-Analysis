{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to check whether there is wrong report file in folder OK\n",
    "# script to detect which \"kesan\" is the conclusion OK\n",
    "# script to detect aggregated report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file(filepath: str):\n",
    "    if filepath.endswith('TOTAL.txt'):\n",
    "        pass\n",
    "    else:\n",
    "        return filepath.lower().endswith('.txt')\n",
    "\n",
    "def remove_punctuations(text: str):\n",
    "    punctuations = '''!()[]{};:'\"\\,<>?@#$%^&*~.''' # / is not removed for fraction, - is not removed for range (ex: l3-l4)\n",
    "    new_text = ''\n",
    "    for char in text:\n",
    "        if char not in punctuations: \n",
    "            new_text += char\n",
    "    return new_text\n",
    "\n",
    "def check_kesan(text: str):\n",
    "    # split the text to each line and strip the space\n",
    "    text = [t.lower().strip() for t in text.splitlines() if t]\n",
    "    first_words = [sentence.split()[0] for sentence in text if len(sentence) != 0]\n",
    "    # check for kesan at beginning of sentence\n",
    "    if 'kesan:' not in text:\n",
    "        if 'kesan' not in text:\n",
    "            # there is possibility for kesan to not be in a newline on its own (combined w other words, so check if kesan at beginning of sentence\n",
    "            if 'kesan' not in first_words:\n",
    "                return True\n",
    "        \n",
    "def check_misplaced_report(text: str, body_part: str):\n",
    "    text = text.lower().split()\n",
    "    body_part = body_part.split('_')\n",
    "    joined_body_part = ''.join(body_part)\n",
    "    # exception for abdomen\n",
    "    if body_part[0] == 'abodemen':\n",
    "        body_part = ['abdomen']\n",
    "\n",
    "    # handle special cases\n",
    "    special_cases = {\n",
    "        'antebrachii': ['antebrachi'],\n",
    "        'clavicualkanan': ['clavicula', 'kanan'],\n",
    "        'ossacrococcygeus': ['os', 'sacro-', 'coccygeus'],\n",
    "        'schedel4': ['schedel/cranium'],\n",
    "        'vertebralumbosakral': ['vertebra', 'lumbosacral'],\n",
    "        'vertebrathoracolumbal': ['vertebra', 'thoracal'],\n",
    "        'bahukanankiri': ['kanan', 'kiri']\n",
    "    }\n",
    "\n",
    "    if joined_body_part in special_cases.keys():\n",
    "        alternative_body_part = special_cases[joined_body_part]\n",
    "        if joined_body_part == 'bahukanankiri':\n",
    "            for word in body_part:\n",
    "                if word in text and 'bahu' in text:\n",
    "                    return False\n",
    "        else:\n",
    "            for word in body_part:\n",
    "                if word not in text:\n",
    "                    for alt_word in alternative_body_part:\n",
    "                        if alt_word not in text:\n",
    "                            if joined_body_part not in text:\n",
    "                                return True\n",
    "                    return False\n",
    "                        \n",
    "    # loop through word and check if word in text\n",
    "    for word in body_part:\n",
    "        if word not in text:\n",
    "            if joined_body_part not in text:\n",
    "                return True\n",
    "                \n",
    "        \n",
    "def check_aggregated_report(text: str):\n",
    "    num_count = sum(1 for line in text.splitlines() if line.strip().split() and line.strip().split()[0].isdigit())\n",
    "    if num_count > 1:\n",
    "        return True\n",
    "    \n",
    "def generate_txt_file(compiled_dict: dict, path: str):\n",
    "    # Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    # # change permission into read, write, execute\n",
    "    # Path(path).chmod(0o777)\n",
    "    for key, val in compiled_dict.items():\n",
    "        with open(path, 'a') as w:\n",
    "            w.write(f\"{key}\\n{val}\\n\\n\")\n",
    "\n",
    "def generate_txt_file_from_list(compiled_list: list, path: str):\n",
    "    with open(path, 'a') as w:\n",
    "        for item in compiled_list:\n",
    "            w.write(f\"{item}.txt\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing txt report in [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['2401100109_00001-1.jpg', '2401100109_00002-2.jpg'], [], [], [], [], [], [], [], [], ['2310180134_Series_1001_0000-1.jpg', '2310200085_Series_1001_0000-2.jpg', '2310200092_Series_1001_0000-3.jpg', '2310200095_Series_1001_0000-4.jpg', '2310200118_Series_1001_0000-5.jpg', '2310200119_Series_1001_0000-6.jpg', '2310200120_Series_1001_0000-7.jpg', '2310200122_Series_1001_0000-8.jpg', '2310200123_Series_1001_0000-9.jpg', '2310200125_Series_1001_0000-10.jpg', '2310200126_Series_1001_0000-11.jpg', '2310200127_Series_1001_0000-12.jpg', '2310200129_Series_1001_0000-13.jpg', '2310200130_Series_1001_0000-14.jpg', '2310200131_Series_1001_0000-15.jpg', '2310200132_Series_1001_0000-16.jpg', '2310200133_Series_1001_0000-17.jpg', '2310200134_Series_1002_0000-18.jpg', '2310200135_Series_1001_0000-19.jpg', '2310200136_Series_1001_0000-20.jpg', '2310200137_Series_1001_0000-21.jpg', '2310200138_Series_1001_0000-22.jpg', '2310200139_Series_1001_0000-23.jpg', '2310200142_Series_1001_0000-24.jpg', '2310200143_Series_1001_0000-25.jpg', '2310200144_Series_1001_0000-26.jpg', '2310200148_Series_1001_0000-27.jpg', '2310200149_Series_1001_0000-28.jpg', '2310200151_Series_1001_0000-29.jpg', '2310200152_Series_1001_0000-30.jpg', '2310200153_Series_1001_0000-31.jpg', '2310200154_Series_1001_0000-32.jpg', '2310200155_Series_1001_0000-33.jpg', '2310200156_Series_1001_0000-34.jpg', '2310200158_Series_1001_0000-35.jpg', '2310200159_Series_1001_0000-36.jpg', '2310210073_Series_1001_0000-37.jpg', '2310210085_Series_1002_0000-38.jpg'], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.path.join('..','datasets','radiology_clean_final')\n",
    "missing_kesan_txt_dir = os.path.join('..','report_analysis','missing_kesan.txt')\n",
    "all_body_part_missing_kesan_txt_dir = os.path.join('..','report_analysis','all_files_missing_kesan.txt')\n",
    "misplaced_category_txt_dir = os.path.join('..','report_analysis','misplaced_category.txt')\n",
    "aggregated_report_txt_dir = os.path.join('..','report_analysis','aggregated_report.txt')\n",
    "\n",
    "missing_kesan = {}\n",
    "misplaced_category = {}\n",
    "aggregated_report = {}\n",
    "all_missing_kesan = []\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        filename = os.path.join(root, file)\n",
    "        body_part = Path(filename).parent.parent.name.lower()\n",
    "        \n",
    "        # if txt file (report)\n",
    "        if txt_file(filename):\n",
    "            f = open(filename, \"r\", encoding=\"unicode_escape\")\n",
    "            text = f.read()\n",
    "            text = remove_punctuations(text)\n",
    "            \n",
    "            # check whether kesan is present in text\n",
    "            if check_kesan(text):\n",
    "                missing_kesan.setdefault(body_part, []).append(file[:-4])\n",
    "                all_missing_kesan.append(file[:-4])\n",
    "            \n",
    "            if check_misplaced_report(text, body_part):\n",
    "                misplaced_category.setdefault(body_part, []).append(file[:-4])\n",
    "\n",
    "            if check_aggregated_report(text):\n",
    "                aggregated_report.setdefault(body_part, []).append(file[:-4])\n",
    "\n",
    "# print(f\"Missing kesan in {kesan_file}\")\n",
    "# print(f\"Misplaced report in {misplaced_category}\")\n",
    "# print(f\"Aggregated report in {aggregated_report}\")\n",
    "                \n",
    "# generate_txt_file(missing_kesan, missing_kesan_txt_dir)\n",
    "# generate_txt_file(misplaced_category, misplaced_category_txt_dir)\n",
    "generate_txt_file(z, aggregated_report_txt_dir)\n",
    "# generate_txt_file_from_list(all_missing_kesan, all_body_part_missing_kesan_txt_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
